{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as function\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics import Accuracy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 16)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "ctr_data = pd.read_csv('avazu.csv')\n",
    "print(ctr_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'click', 'hour', 'C1', 'banner_pos', 'device_type',\n",
      "       'device_conn_type', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20',\n",
      "       'C21', 'device_model_int'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(ctr_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop column 'Unnamed: 0'\n",
    "ctr_data = ctr_data.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "click               0\n",
      "hour                0\n",
      "C1                  0\n",
      "banner_pos          0\n",
      "device_type         0\n",
      "device_conn_type    0\n",
      "C14                 0\n",
      "C15                 0\n",
      "C16                 0\n",
      "C17                 0\n",
      "C18                 0\n",
      "C19                 0\n",
      "C20                 0\n",
      "C21                 0\n",
      "device_model_int    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check missing values or null or nan values\n",
    "print(ctr_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   click      hour    C1  banner_pos  device_type  device_conn_type    C14  \\\n",
      "0      0  14102100  1005           0            1                 2  15706   \n",
      "1      0  14102100  1005           0            1                 0  15704   \n",
      "2      0  14102100  1005           0            1                 0  15704   \n",
      "3      0  14102100  1005           0            1                 0  15706   \n",
      "4      0  14102100  1005           1            1                 0  18993   \n",
      "\n",
      "   C15  C16   C17  C18  C19     C20  C21     device_model_int  \n",
      "0  320   50  1722    0   35      -1   79 -4536565594672005814  \n",
      "1  320   50  1722    0   35  100084   79   -80052322344914806  \n",
      "2  320   50  1722    0   35  100084   79 -3130634972019121531  \n",
      "3  320   50  1722    0   35  100084   79 -8587292268327570678  \n",
      "4  320   50  2161    0   35      -1  157 -7699311560514132401  \n"
     ]
    }
   ],
   "source": [
    "print(ctr_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count   Dtype\n",
      "---  ------            --------------   -----\n",
      " 0   click             100000 non-null  int64\n",
      " 1   hour              100000 non-null  int64\n",
      " 2   C1                100000 non-null  int64\n",
      " 3   banner_pos        100000 non-null  int64\n",
      " 4   device_type       100000 non-null  int64\n",
      " 5   device_conn_type  100000 non-null  int64\n",
      " 6   C14               100000 non-null  int64\n",
      " 7   C15               100000 non-null  int64\n",
      " 8   C16               100000 non-null  int64\n",
      " 9   C17               100000 non-null  int64\n",
      " 10  C18               100000 non-null  int64\n",
      " 11  C19               100000 non-null  int64\n",
      " 12  C20               100000 non-null  int64\n",
      " 13  C21               100000 non-null  int64\n",
      " 14  device_model_int  100000 non-null  int64\n",
      "dtypes: int64(15)\n",
      "memory usage: 11.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ctr_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "click                  2\n",
      "hour                   1\n",
      "C1                     6\n",
      "banner_pos             5\n",
      "device_type            4\n",
      "device_conn_type       4\n",
      "C14                  420\n",
      "C15                    5\n",
      "C16                    6\n",
      "C17                  128\n",
      "C18                    4\n",
      "C19                   37\n",
      "C20                  137\n",
      "C21                   29\n",
      "device_model_int    2473\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "n_unique_values = ctr_data.nunique()\n",
    "print(n_unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "click\n",
      "0    82510\n",
      "1    17490\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# value counts of target column\n",
    "print(ctr_data['click'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ctr_data.drop(columns=['click'])\n",
    "target = ctr_data['click']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 14) (80000,)\n",
      "(20000, 14) (20000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42, stratify=target)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Scaling features:\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it\n",
    "scaled_X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sample: tensor([ 0.0000, -0.0324, -0.4937, -0.0962, -0.3146,  0.8222, -1.5181,  5.2832,\n",
      "         0.9291,  0.9894, -0.3796,  1.2805, -1.4432,  0.8140])\n",
      "label sample: tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "#3. Converting to Pytorch Tensors - TensorDataset\n",
    "\n",
    "#Instantiate dataset class\n",
    "dataset = TensorDataset(torch.tensor(scaled_X_train, dtype=torch.float32), torch.tensor(y_train.values, dtype=torch.float32).reshape(-1,1))\n",
    "\n",
    "#Access an individual sample\n",
    "input_sample, label_sample = dataset[0]\n",
    "print('input sample:', input_sample)   \n",
    "print('label sample:', label_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. DataLoader\n",
    "batch_size = 32\n",
    "suffle = True\n",
    "\n",
    "# Create a DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=suffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Transform data and convert tensors with test data\n",
    "\n",
    "# Fit the scaler to the data and transform it\n",
    "scaled_test_features = scaler.transform(X_test) #transform not fit \n",
    "\n",
    "#Instantiate dataset class\n",
    "test_dataset = TensorDataset(torch.tensor(scaled_test_features, dtype=torch.float32), torch.tensor(y_test.values, dtype=torch.float32).reshape(-1,1))\n",
    "\n",
    "#DataLoader\n",
    "batch_size = 32\n",
    "suffle = True\n",
    "\n",
    "# Create a DataLoader\n",
    "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=suffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary classification model\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(num_features, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=14, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (5): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5584\n",
      "Epoch 2/10, Loss: 0.4767\n",
      "Epoch 3/10, Loss: 0.4621\n",
      "Epoch 4/10, Loss: 0.4551\n",
      "Epoch 5/10, Loss: 0.4503\n",
      "Epoch 6/10, Loss: 0.4469\n",
      "Epoch 7/10, Loss: 0.4443\n",
      "Epoch 8/10, Loss: 0.4424\n",
      "Epoch 9/10, Loss: 0.4409\n",
      "Epoch 10/10, Loss: 0.4398\n"
     ]
    }
   ],
   "source": [
    "#1. Train the model\n",
    "num_epochs = 10  # Number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Training mode\n",
    "    training_loss = 0.0\n",
    "\n",
    "    for feature, target in data_loader:\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        pred = model(feature)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        training_loss += loss.item()\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    epoch_loss = training_loss / len(data_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=14, out_features=32, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (5): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Calculating validation loss\n",
    "\n",
    "validation_loss = 0.0\n",
    "model.eval() #Put model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        #Run the forward pass\n",
    "        outputs = model(inputs)\n",
    "        #Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        validation_loss += loss.item()\n",
    "\n",
    "epoch_loss = validation_loss / len(val_loader)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4386\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8251\n"
     ]
    }
   ],
   "source": [
    "#3. Evaluation Accuracy with TorchMetric\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "acc = Accuracy(task='binary')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        predictions = (outputs > 0.5).float()  # Convert probabilities to binary predictions\n",
    "        acc.update(predictions, labels.int())\n",
    "\n",
    "accuracy = acc.compute()\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
